<웹 계층에서 부하를 분산하는 다양한 방식들>

# 이 글에 대해서

시스템 설계자라면 누구나 직면하는 질문이 있다.

> 웹 애플리케이션이 커지면 트래픽 부하를 어떻게 처리할 것인가?

Alex Xu의 유명한 "System Design Interview" 1권의 첫 번째 챕터가 다루고 있는 주제 역시 **규모 확장성**이다. 시스템 설계에서 가장 중요한 주제 중 하나로, 특히 **부하 분산**은 웹 애플리케이션의 성장과 안정성에 핵심적인 요소일 것이다.

이 글에서는 **웹 계층의 부하 분산**에 초점을 맞추어, 웹 계층에서의 로드밸런싱을 통해 트래픽을 분산하는 전략에 대해 다룬다. 특히 **스프링 클라우드**, **쿠버네티스**, **AWS**와 같은 다양한 환경에서 부하 분산이 어떻게 구현되는지를 살펴보고, **L4와 L7 로드밸런서**의 차이를 명확히 이해하고자 한다. 또한, 실제 시스템 설계에서 자주 발생하는 질문들과 그에 대한 답을 찾아보며, 부하 분산을 위한 최적의 접근 방식을 탐구해보자.


# AWS vs 쿠버네티스 vs 스프링클라우드

## 1. AWS에서의 로드밸런싱
AWS는 다양한 수준에서의 **Elastic Load Balancer (ELB)**를 제공하는데, 각각의 로드 밸런서는 L4 또는 L7에서 트래픽을 처리할 수 있다.

- **Application Load Balancer (ALB)**: L7 로드밸런서로서 **HTTP/HTTPS 트래픽**에 최적화되어 있으며, URL 경로와 헤더 등을 기준으로 라우팅을 제공한다.
- **Network Load Balancer (NLB)**: L4 레벨에서 **TCP/UDP** 트래픽을 처리하며, 빠른 응답과 높은 처리량이 필요한 경우에 사용된다.
- **API Gateway**: 서버리스로 작동하며, **HTTP 요청**을 여러 마이크로서비스로 라우팅하여 간단하게 API 트래픽을 관리할 수 있는 게이트웨이 서비스이다.

AWS는 이러한 로드밸런서를 통해 클라우드 인프라에서 트래픽 부하를 효율적으로 분산하고 **고가용성**을 보장한다.

## 2. 쿠버네티스에서의 로드밸런싱
**Kubernetes**는 클러스터 내부에서의 **트래픽 분산**과 **서비스 확장성**을 중점으로 제공한다.

- **Service 객체와 Ingress**를 통해 클러스터 내부의 Pod들 간 트래픽을 균형 있게 나누며, 외부로부터의 요청을 적절히 라우팅한다.
- **Ingress Controller**는 HTTP/HTTPS 요청을 처리하며, 여러 마이크로서비스로의 라우팅을 지원하여 클러스터 내부의 트래픽을 관리한다.

Kubernetes는 클러스터에서 **자동 확장**과 유연한 트래픽 관리 기능을 통해 **컨테이너화된 애플리케이션**을 관리하고 있다.

## 3. Spring Cloud Gateway와 로드밸런싱
**Spring Cloud Gateway**는 **API Gateway**로 사용되며, 애플리케이션 계층에서의 **요청 라우팅**과 **부하 분산**을 제공한다.

- **라우팅 및 필터링**: 특정 경로별로 요청을 라우팅하고, API 호출에 대해 인증, 필터링 등을 수행한다.
- **간단한 부하 분산**: **라운드 로빈** 방식을 통해 여러 마이크로서비스로 요청을 분산하지만, 주로 API의 진입점으로서의 역할에 중점을 둔다.

Spring Cloud Gateway는 **애플리케이션 레벨**에서 트래픽 관리 및 정책 적용을 중심으로 동작한다.



| 특징                           | **AWS**                                 | **Kubernetes**                              | **Spring Cloud Gateway**                   |
|--------------------------------|-----------------------------------------|---------------------------------------------|--------------------------------------------|
| **주요 역할**                  | 클라우드 인프라에서 트래픽 부하 분산      | 컨테이너 관리와 서비스 트래픽 로드밸런싱     | API 라우팅 및 기본적인 로드밸런싱           |
| **로드밸런싱 수준**            | L4/L7 네트워크 수준 로드밸런싱            | 클러스터 내부 서비스와 외부 연결            | 애플리케이션 레벨                           |
| **로드밸런싱 구현 방식**       | ELB (ALB, NLB), API Gateway 사용         | Ingress Controller, Service를 통한 분산     | 주로 라운드 로빈 방식                       |
| **확장성**                     | Auto Scaling, 여러 로드밸런서 사용       | Auto Scaling, Pod 수 증가                   | API Gateway로 확장성 제공                   |
| **트래픽 관리**                | 다양한 종류의 로드밸런서 및 API Gateway  | HTTP/HTTPS, TCP/UDP 트래픽 처리 (Ingress)   | 서비스 간 라우팅과 인증, 필터링             |



# 각 기술의 로드밸런싱 관점과 역할 차이

## 1. AWS, Kubernetes, Spring Cloud Gateway: 각 기술이 바라보는 로드밸런싱의 관점은?

AWS, Kubernetes, 그리고 Spring Cloud Gateway는 모두 로드밸런싱을 담당하지만, 각 기술은 **다른 레벨과 관점**에서 트래픽을 관리한다. 

AWS는 **클라우드 인프라 레벨**에서 트래픽의 효율적 분배와 확장성을 지원하며, 사용자는 글로벌 규모의 부하 분산과 자동화를 경험할 수 있다.

Kubernetes는 **컨테이너 오케스트레이션 플랫폼**으로서, 클러스터 내부의 트래픽 관리와 Pod의 확장에 중점을 두고 있다. 

Spring Cloud Gateway는 **애플리케이션 계층**에서 API 요청을 **통합적으로 관리**하는 데 주안점을 두며, 세부적인 API 경로와 서비스 간의 라우팅을 중점적으로 처리한다.

## 2. 인프라 레벨 vs 애플리케이션 레벨의 부하 분산

- **AWS의 로드밸런싱**은 인프라 자체에서 이루어지는 고성능의 트래픽 분산을 의미한다. **ALB**와 **NLB**는 각각 L7과 L4 계층에서의 요청을 관리하며, 대규모 트래픽을 수평적으로 확장하는 데 최적화되어 있다. 이와 같은 **인프라 레벨**의 로드밸런싱은 트래픽의 글로벌 분산과 높은 가용성을 보장한다.
- **Kubernetes**는 **컨테이너 관리와 클러스터 내 트래픽 분산**에 초점을 맞추며, **Service**와 **Ingress Controller**를 통해 클러스터 내부의 여러 Pod에 대한 부하를 분산한다. 이는 애플리케이션이 물리적 서버 또는 클러스터 내에서 효과적으로 분산될 수 있도록 지원하며, **내부 트래픽 처리**와 **Pod 간 균형**을 맞추는 역할을 수행한다.
- **Spring Cloud Gateway**는 애플리케이션에 대한 **API 진입 지점**을 통합하고, 마이크로서비스 간의 라우팅, 인증, 속도 제한 등의 기능을 제공한다. 이는 **API 레벨에서의 세부적인 부하 분산**과 **정책 적용**에 중점을 두며, 각 마이크로서비스가 독립적으로 개발 및 운영될 수 있도록 돕는다.


## 3. 각 기술의 역할 협력: 로드밸런싱의 계층적 구조

실제 시스템에서는 AWS, Kubernetes, Spring Cloud Gateway가 각각의 역할을 맡아 **다계층의 부하 분산**을 구현한다.

AWS의 로드밸런서는 **외부에서 클라우드로의 트래픽 분산**을 담당하고, Kubernetes는 **클러스터 내에서의 Pod 부하 분산**을 수행하며, Spring Cloud Gateway는 **마이크로서비스 간의 트래픽을 라우팅**한다.

이러한 다계층 구조는 트래픽이 AWS에서 클러스터로, 그리고 애플리케이션 내의 마이크로서비스로 **점진적으로 세분화**되며 분산될 수 있도록 한다.


#### 4. 복합적인 사용의 장점: 어떤 시나리오에 적합할까?

각 기술의 로드밸런싱 접근 방식을 복합적으로 사용하면 다양한 **비즈니스 요구**와 **성능 목표**를 충족할 수 있다.

- **고성능 트래픽 관리**: AWS의 ELB는 글로벌 규모의 대량 트래픽을 신속하게 처리하며, 사용자가 어디에 위치하든 가장 가까운 리전으로 라우팅할 수 있도록 지원한다.
- **세부적인 클러스터 트래픽 관리**: Kubernetes는 애플리케이션 내의 **세부적인 부하 분산**을 처리하고, Pod의 상태에 따라 자동으로 확장되거나 축소될 수 있어 클러스터 수준에서의 유연한 트래픽 관리가 가능하다.
- **정교한 API 정책 관리**: Spring Cloud Gateway는 API 호출마다 적용되는 **정교한 인증과 정책 관리**를 통해 애플리케이션의 보안과 기능적 유연성을 강화한다. 이는 대규모 마이크로서비스 아키텍처에서 **안전하고 일관된 트래픽 흐름**을 유지하는 데 필수적이다.

#### 5. 각 기술이 담당하는 특화된 역할과 그 필요성

- **AWS**: 글로벌 리치와 **인프라 자동화**를 통한 확장성. 클라우드 인프라에서의 트래픽 관리와 **고가용성**을 위한 핵심 요소.
- **Kubernetes**: **컨테이너화된 마이크로서비스**를 관리하고 Pod의 트래픽을 균형있게 분산하는 데 필수적. 이는 애플리케이션의 수평적 확장을 용이하게 한다.
- **Spring Cloud Gateway**: **API 요청의 일관성 관리** 및 정책 적용을 통해 각 서비스의 독립성과 보안을 확보하며, 마이크로서비스 아키텍처에서 **서비스 간 라우팅**을 간편하게 한다.


# Spring Cloud Gateway, Kubernetes, AWS를 함께 사용한다면?

실제 분산 어플리케이션 아키텍처를 구성할 때, 이들을 함께 사용할 수 있을까? 어떤 **구체적인 유스케이스**를 생각해볼 수 있을까?
 

## 1. 유스케이스: 글로벌 e-커머스 플랫폼

글로벌 사용자 기반을 가진 **e-커머스 플랫폼**을 생각해보자. 

이 플랫폼은 **제품 조회**, **장바구니 관리**, **결제 처리**와 같은 서비스를 제공하며, 수많은 사용자 요청을 적절히 처리하고 시스템의 가용성을 보장해야 할 것이다.

이때, 각각의 역할과 기능이 어떻게 사용될지 살펴보자. 

## 2. AWS의 역할: 글로벌 트래픽 분산과 인프라 관리
- **Application Load Balancer (ALB)**는 외부 사용자의 HTTP/HTTPS 요청을 받아 AWS 내부의 적절한 **Kubernetes 클러스터**로 분산한다. 이 ALB는 **L7 로드밸런싱**을 제공하여 URL 경로나 HTTP 헤더를 기반으로 **리전별 트래픽 관리**를 수행한다.
- **Network Load Balancer (NLB)**는 **TCP/UDP** 기반의 높은 처리량을 필요로 하는 요청을 처리하며, **결제 시스템**과 같이 지연 시간이 중요한 요청에 사용된다.
- 이러한 AWS의 로드밸런서는 트래픽의 첫 번째 진입점에서 **인프라 수준의 부하 분산**을 수행하고, 글로벌 확장성과 **고가용성**을 보장한다.

## 3. Kubernetes의 역할: 클러스터 내부 트래픽 관리와 서비스 확장성
- AWS의 **ALB**에서 전달된 요청은 **Kubernetes 클러스터** 내부의 **Ingress Controller**로 들어온다. Ingress Controller는 HTTP/HTTPS 요청을 관리하며, 이를 클러스터 내부의 적절한 **서비스(Pod)**로 라우팅한다.
- **Kubernetes Service**는 각 마이크로서비스 간 트래픽을 분산하며, 클러스터 내에서 **라운드 로빈** 방식으로 요청을 여러 Pod에 분배한다. 이를 통해 클러스터 내부의 트래픽이 균형 있게 분산되고, **자동 확장**을 통해 Pod의 수를 동적으로 조정하여 부하를 감당한다.
- 트래픽이 급격히 증가할 경우 Pod 수를 자동으로 늘려서 **확장성**을 확보해야 한다. 이때 핵심적인 역할을 하는 것이 Kubernetes의 **Auto Scaling** 기능이다.

## 4. Spring Cloud Gateway의 역할: 애플리케이션 계층에서의 세부 라우팅과 API 관리
- **Spring Cloud Gateway**는 Kubernetes 클러스터 내부에서 **API Gateway**로서 기능한다. 모든 사용자 요청은 먼저 Spring Cloud Gateway를 거쳐 각 마이크로서비스로 라우팅된다. 이는 **API 호출의 중앙 진입점** 역할을 하며, 특정 API 경로별로 요청을 적절한 서비스로 전달한다.
- Spring Cloud Gateway는 **JWT 인증**, **요청 필터링**, **속도 제한** 등 다양한 정책을 적용하여, 각 마이크로서비스가 보안적으로 안전하게 요청을 받을 수 있도록 한다. 예를 들어, 특정 API 요청이 인증되지 않았거나 정책에 맞지 않는 경우 이를 **차단하거나 리다이렉트**할 수 있다.
- Spring Cloud Gateway는 API 요청을 라우팅하는 동시에 **A/B 테스트**나 **API 버저닝**과 같은 기능을 통해 **서비스 유연성**을 높인다. 예를 들어, `/api/v1/products`와 `/api/v2/products` 경로로 들어오는 요청을 각각 다른 버전의 서비스로 라우팅함으로써 새로운 기능 테스트를 진행할 수 있다.

## 5. 스텝별로 정리해보자면

- **1단계: 글로벌 트래픽 처리 (AWS)**  
  AWS의 **ALB**는 글로벌 사용자 요청을 수신하고, 이를 **적절한 리전**의 Kubernetes 클러스터로 라우팅한다. 이 과정에서 ALB는 L7 로드밸런싱을 사용하여 HTTP/HTTPS 요청을 분석하고, 사용자에게 가장 가까운 리전으로 트래픽을 분산한다.

- **2단계: 클러스터 내부 부하 분산 (Kubernetes)**  
  AWS에서 전달된 요청은 Kubernetes의 **Ingress Controller**로 들어와, 클러스터 내부의 여러 **Pod**로 요청을 분배한다. 이때, Kubernetes는 **Pod 간 균형있는 부하 분산**을 수행하며, 필요시 새로운 Pod를 생성하여 트래픽 부하를 감당할 수 있도록 한다.

- **3단계: API 관리와 세부 라우팅 (Spring Cloud Gateway)**  
  클러스터 내부의 트래픽은 **Spring Cloud Gateway**를 통해 각 마이크로서비스로 라우팅된다. 이 Gateway는 API 호출마다 **인증, 필터링, 속도 제한**을 수행하며, API 요청의 **중앙 진입점**으로 작동한다. 

## 6. 요약 

AWS는 **글로벌 트래픽 관리**와 인프라 확장성을, Kubernetes는 **클러스터 내부의 확장성과 Pod 간 부하 분산**을, Spring Cloud Gateway는 **API 진입점 관리와 서비스 간 트래픽 제어**를 담당한다. 

## 그런데... 굳이 이렇게 많은 스텝이 필요할까? 

상황에 따라 위에서 설명한 AWS, Kubernetes, Spring Cloud Gateway를 모두 사용할 필요가 없고 일부를 단순화하거나 생략할 수 있을 것이다.

- **넷플릭스 같은 글로벌 서비스라면**: 넷플릭스와 같이 **하루 DAU(Daily Active Users)가 수백만 명**에 달하는 서비스는 전 세계에 걸쳐 사용자에게 빠른 응답성을 제공해야 한다. 이런 경우 **AWS의 글로벌 인프라와 ALB**를 사용하여 각 리전별로 트래픽을 효율적으로 분산하고, **Kubernetes**로 자동 확장을 통해 급격한 트래픽 변동에 대응하며, **Spring Cloud Gateway**로 애플리케이션 레벨에서 세부적인 요청 관리를 하는 것이 적합하다. 사용자 위치에 따라 가장 가까운 서버에서 응답을 제공하여 **지연 시간을 최소화** 해야 하는 니즈가 있기 때문이다.

- **국내 유저 중심의 대기업 서비스라면**: 예를 들어 **하루 DAU가 10만 명에서 100만 명 사이**인 국내 중심의 대기업 서비스라면, 글로벌 인프라의 필요성은 낮다. 이 경우 **AWS의 특정 리전**에 Kubernetes 클러스터를 구성하고, 필요에 따라 **ALB**나 **Kubernetes Ingress**로 트래픽을 분산하는 것이 충분할 것이다. 이와 함께, **Spring Cloud Gateway** 대신 **AWS API Gateway**를 사용해 간단하게 API를 관리할 수 있다. 

- **중견 기업 수준의 서비스라면**: **DAU가 1만 명 이하**의 중소규모 서비스라면 모든 컴포넌트를 사용할 필요가 없다. AWS **ALB**와 같은 간단한 로드밸런싱과 **ECS** 또는 **EC2 인스턴스**를 사용해 서비스를 운영하는 것이 효율적일 수 있다. **Kubernetes**를 생략하고, 컨테이너 오케스트레이션 없이 EC2에서 직접 애플리케이션을 운영하는 것도 고려해볼 만하다. 또한 **Spring Cloud Gateway** 대신 간단한 **라우팅 로직**을 서버 내부에 구현함으로써 운영 복잡성을 줄일 수 있다.

--- 

# Q&A

## L4, L7?

**L4**와 **L7**은 네트워크에서 트래픽을 처리하는 **OSI 7계층 모델**의 각각의 레이어를 의미한다. 이 용어들은 로드밸런서가 트래픽을 처리하는 **수준**을 나타내며, 이에 따라 기능과 성능이 크게 달라진다.

### L4 로드밸런서: Transport Layer

**L4 로드밸런서**는 **전송 계층(Transport Layer)**에서 트래픽을 처리한다. 주로 **IP 주소**와 **포트 번호**를 기반으로 트래픽을 라우팅하며, 데이터의 내용에는 관심을 두지 않는다. **TCP**와 **UDP** 같은 프로토콜을 사용하며, 패킷 수준에서 작동하기 때문에 **빠르고 효율적**이다. 예를 들어, **AWS의 Network Load Balancer (NLB)**는 높은 처리량과 낮은 지연 시간으로 대량의 트래픽을 처리하는 데 최적화되어 있다.

### L7 로드밸런서: Application Layer

**L7 로드밸런서**는 **애플리케이션 계층(Application Layer)**에서 트래픽을 처리한다. **HTTP 헤더**, **URL 경로**, **쿠키** 등 애플리케이션 데이터를 기반으로 트래픽을 라우팅하기 때문에 더 **정교하고 지능적인 라우팅**이 가능하다. 예를 들어, 요청 경로에 따라 다른 서버에 트래픽을 보내거나, 헤더 정보에 따라 특정 서비스로 분배하는 식이다. **AWS의 Application Load Balancer (ALB)**가 대표적인 예로, 주로 **웹 애플리케이션**이나 **API 서비스**에서 사용된다.

### L4와 L7의 차이와 선택 기준
- **L4 로드밸런서**는 **빠른 처리 속도**와 **낮은 오버헤드**가 장점으로, 성능이 중요한 경우에 적합하다. 예를 들어, 실시간 데이터 처리나 금융 거래와 같이 대규모 트래픽을 효율적으로 전달해야 할 때 사용된다.
- **L7 로드밸런서**는 **애플리케이션 데이터**를 분석하고 **정밀한 라우팅**이 필요할 때 사용된다. 예를 들어, URL 경로에 따라 다른 서버로 트래픽을 보내야 하는 웹 애플리케이션에서 적합하다.

따라서, **고성능과 낮은 지연**이 중요한 경우에는 **L4 로드밸런서**를, **복잡한 라우팅과 애플리케이션 분석**이 필요한 경우에는 **L7 로드밸런서**를 선택하는 것이 좋다.

### L4와 L7의 비교

| 특성                        | **L4 (Transport Layer)**                          | **L7 (Application Layer)**                           |
|-----------------------------|---------------------------------------------------|------------------------------------------------------|
| **트래픽 처리 기준**        | IP 주소 및 포트 번호                                | HTTP 헤더, URL 경로, 쿠키, 요청 본문 등               |
| **처리 속도**               | 빠르고 효율적, 낮은 오버헤드                        | 상대적으로 느림, 복잡한 분석 및 라우팅 가능           |
| **사용 시나리오**           | 단순 트래픽 라우팅, 성능이 중요한 경우               | 애플리케이션 수준 라우팅, 세분화된 요청 처리 필요 시  |
| **지원 프로토콜**           | TCP, UDP 등                                        | HTTP, HTTPS, WebSocket 등                             |
| **AWS 로드밸런서 예시**     | **Network Load Balancer (NLB)**                    | **Application Load Balancer (ALB)**                   |




## 그러면 인프라 수준에서 L7을 두고, 어플리케이션 수준에서 마이크로 서비스 관리를 위한 게이트웨이를 두는 것도 가능하며 일반적인가? 또한 게이트웨이도 인스턴스를 여러 개 두기도 하나?

그렇다! 인프라 수준에서 **L7 로드밸런서를 두고, 애플리케이션 수준에서 API Gateway (Spring Cloud Gateway 등)를 사용하는 구성**은 **매우 일반적이며 널리 사용되는 패턴**이다. 

이는 클라우드 네이티브 아키텍처에서 **확장성**, **유연성**, 그리고 **관리 효율성**을 모두 확보하기 위한 최적의 방식으로 간주된다. 각각의 계층에서 역할을 분리하여 다음과 같은 이점을 누릴 수 있다.


### 1. 인프라 수준의 L7 로드밸런서 (예: AWS ALB)
**L7 로드밸런서**(예: AWS의 Application Load Balancer)는 **인프라 계층**에서 **트래픽을 효율적으로 관리하고 분산**하는 역할을 수행한다. 주요 특징과 이점은 다음과 같다.

- **외부 트래픽 관리**: L7 로드밸런서는 사용자로부터 들어오는 트래픽을 여러 백엔드 인스턴스로 분산시킨다. 즉, 인터넷으로부터 들어오는 요청을 클러스터 내부의 여러 서비스로 적절히 나누어 **부하를 분산**하는 역할을 한다.
- **고가용성 및 자동 확장**: L7 로드밸런서는 트래픽 양에 따라 백엔드 서비스의 인스턴스를 **자동으로 스케일링**하고, 이를 통해 가용성을 보장한다. 클라우드 제공자가 관리하는 로드밸런서를 사용하면, 이러한 **자동화된 인프라 관리**의 이점을 누릴 수 있다.
- **SSL 종료**: SSL 인증서 관리를 통해 **HTTPS 트래픽을 처리**하고, 트래픽을 백엔드로 전달할 때는 HTTP로 전환하여 백엔드 서비스의 부담을 줄이는 SSL 종료 기능을 제공한다. 이를 통해 백엔드 인스턴스가 SSL 처리에서 자유로워져 성능이 개선된다.

### 2. 애플리케이션 수준의 API Gateway (예: Spring Cloud Gateway)
**API Gateway**는 애플리케이션 계층에서 **마이크로서비스 아키텍처**를 효율적으로 관리하고, 여러 서비스로 트래픽을 라우팅하는 역할을 한다. 주요 기능과 이점은 다음과 같다.

- **통합된 진입 지점**: API Gateway는 모든 외부 요청에 대해 **통합된 진입 지점**을 제공하여 마이크로서비스 아키텍처를 더욱 간단하고 효율적으로 만든다. 각 서비스는 직접 외부에 노출되지 않고, Gateway를 통해서만 접근 가능하다.
- **정책 적용 및 보안**: 인증, 권한 부여, 요청 필터링, 속도 제한 등과 같은 정책을 적용할 수 있다. 예를 들어, 특정 사용자는 특정 API만 사용할 수 있도록 제한하는 **인증 및 권한 부여** 로직을 Gateway에서 관리할 수 있다.
- **유연한 라우팅**: 요청 URL, HTTP 헤더, 파라미터 등의 조건을 기반으로 다양한 백엔드 마이크로서비스로 요청을 라우팅한다. 이를 통해 각 서비스에 맞는 경로로 트래픽을 분배하는 것이 가능하다.
- **추가 기능**: 로깅, 모니터링, 데이터 변환 등의 기능을 제공한다. 이를 통해 개발자는 각각의 마이크로서비스에 직접적인 로깅이나 인증 로직을 넣지 않고, 중앙에서 관리할 수 있다.

### 일반적인 아키텍처에서의 결합
이러한 두 계층을 결합하여 사용하는 아키텍처는 다음과 같은 형태로 구성된다:

1. **외부 클라이언트 → L7 로드밸런서**: 외부의 사용자 또는 클라이언트가 API를 호출하면, **L7 로드밸런서**(예: ALB)가 그 요청을 받는다. 이 로드밸런서는 트래픽을 여러 **API Gateway 인스턴스**로 나누어준다.

2. **L7 로드밸런서 → API Gateway**: 로드밸런서를 통과한 요청은 애플리케이션 수준의 **API Gateway**로 전달된다. 이 단계에서 요청이 어떤 마이크로서비스로 라우팅되어야 하는지 결정된다. 요청의 경로나 헤더 정보를 기반으로 Gateway가 이를 **각각의 마이크로서비스**로 적절히 전달한다.

3. **API Gateway → 마이크로서비스**: 최종적으로 Gateway는 적절한 **마이크로서비스**로 트래픽을 전달하며, 이러한 서비스들은 API Gateway로부터의 요청을 받아 작업을 처리한다.

### 아키텍처의 장점
1. **책임 분리**:
    - **L7 로드밸런서**는 **트래픽 분산**과 **고가용성 보장**에 집중한다. 이는 인프라 수준에서 사용자 요청을 여러 인스턴스로 효율적으로 분산시키는 데 목적이 있다.
    - **API Gateway**는 **라우팅**뿐만 아니라 **보안, 정책, 로깅, 모니터링** 등 애플리케이션 관련 로직을 담당한다. 이를 통해 비즈니스 로직과 API 관리 로직을 분리하여 더 쉽게 관리할 수 있다.

2. **확장성**:
    - **로드밸런서**는 트래픽의 양에 따라 자동으로 인프라의 규모를 확장하고, 다수의 API Gateway 인스턴스에 트래픽을 분배한다.
    - **API Gateway** 자체도 **수평적 확장**이 가능하며, 각각의 마이크로서비스 역시 개별적으로 확장할 수 있어 전체적인 시스템이 매우 확장성 있는 구조를 갖출 수 있다.

3. **중앙 집중식 관리**:
    - 모든 요청이 API Gateway를 거치기 때문에, **정책 관리**나 **모니터링**이 중앙에서 수행될 수 있다. 이를 통해 각 서비스에 일일이 보안 정책을 적용하는 대신, Gateway에서 통합된 보안과 인증 처리가 가능하다.

4. **보안 강화**:
    - 외부 클라이언트는 직접적으로 마이크로서비스와 통신하지 않고, 항상 Gateway를 통해서만 요청을 보낸다. 이로써 각 서비스는 외부 노출을 최소화할 수 있으며, 공격의 표적이 될 가능성도 줄어든다.
    - **SSL 종료** 기능을 L7 로드밸런서에서 수행함으로써, API Gateway와 마이크로서비스의 부담을 줄이고 네트워크의 **보안 트래픽 처리**를 효율적으로 관리할 수 있다.

### 일반적인 패턴의 활용 예
- **AWS 환경**에서 주로 사용되는 패턴으로, **Application Load Balancer (ALB)**는 외부에서 들어오는 모든 HTTP 요청을 받아 여러 **API Gateway**(예: Spring Cloud Gateway)로 분배한다. 이때 API Gateway는 각 요청을 분석하고 필요한 마이크로서비스로 전달한다.
- 이러한 패턴은 **대규모 마이크로서비스 아키텍처**에서 매우 일반적이며, **Netflix**, **Amazon**, **Uber**와 같은 대형 시스템에서 흔히 볼 수 있는 구조이다.





--- 

## 자료 참고  

- https://aws.amazon.com/ko/compare/the-difference-between-the-difference-between-application-network-and-gateway-load-balancing
- https://kubernetes.io/docs/concepts/services-networking/ingress/
- https://aws.amazon.com/ko/blogs/opensource/kubernetes-ingress-aws-alb-ingress-controller/
- https://aws.amazon.com/ko/solutions/case-studies/netflix-reinvent-2019-day-in-the-life-of-a-netflix-engineer/
- https://aws.amazon.com/ko/solutions/case-studies/netflix-case-study/
- https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html
- https://m.blog.naver.com/bizblocklll/222153971491
- https://blog.naver.com/PostView.naver?blogId=ljk041180&logNo=222879664101&noTrackingCode=true